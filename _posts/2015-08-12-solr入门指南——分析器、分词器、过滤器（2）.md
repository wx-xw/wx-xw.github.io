---
layout: post
category: solr
tags: [solr,搜索]

---


## 分词器

分词器是在分析器中，分析器知道整体的效果目标，而分词器只是进行分词。

分词器的输入是一个Reader对象，输出是一个TokenStream对象。

虽然叫tokenizer，但实际上使用的类是实现org.apache.solr.analysis.TokenizerFactory接口的，这个工厂类在必要的时候可以创建一个tokenizer的实例，这个实例必须是继承org.apache.lucene.analysis.TokenStream的类的实例。

### Standard Tokenizer

这个分词器将文本分为词，对于标点符号以及空格，当做分割符用。

- 点（.）后没有紧跟空格，则不会当做分隔符用，点会包含在词中
- @被当做分隔符，所以无法把一个email地址分为一个词
- 连字符也会被当做分隔符

**工厂类**：solr.StandardTokenizerFactory

**参数**

- maxTokenLength：（整数，默认255），分词后的单一词的长度大于maxTokenLength就会忽略这个分词

```xml
<analyzer>
  <tokenizer class="solr.StandardTokenizerFactory"/>
</analyzer>
```

**输入**："Please, email john.doe@foo.com by 03-09, re: m37-xq."

**输出**："Please", "email", "john.doe", "foo.com", "by", "03", "09", "re", "m37", "xq"

### Classic Tokenizer

在solr3.1以及之前的版本，这个分词器和Standard Tokenizer是一样的。

- 点（.）后没有紧跟空格，则不会当做分隔符用，点会包含在词中
- 连字符也会被当做分隔符，除非连字符的任意一端有数字
- 识别域名以及email地址成一个词

**工厂类**：solr.ClassicTokenizerFactory

**参数**：

- maxTokenLength：（整数，默认255），分词后的单一词的长度大于maxTokenLength就会忽略这个分词

```xml
<analyzer>
  <tokenizer class="solr.ClassicTokenizerFactory"/>
</analyzer>
```

**输入**："Please, email john.doe@foo.com by 03-09, re: m37-xq."

**输出**："Please", "email", "john.doe@foo.com", "by", "03-09", "re", "m37-xq"

### Keyword Tokenizer

这个分词器将整个文本域的值当做一个词。

**工厂类**：solr.KeywordTokenizerFactory

**参数**：无

```xml
<analyzer>
  <tokenizer class="solr.KeywordTokenizerFactory"/>
</analyzer>
```

**输入**："Please, email john.doe@foo.com by 03-09, re: m37-xq."

**输出**："Please, email john.doe@foo.com by 03-09, re: m37-xq."

### Letter Tokenizer

这个分词器将文本中的使用所有非字符的字符当做封隔器

**工厂类**：solr.LetterTokenizerFactory

**参数**：无

```xml
<analyzer>
  <tokenizer class="solr.LetterTokenizerFactory"/>
</analyzer>
```

**输入**："I can't."

**输出**："I", "can", "t"

### Lower Case Tokenizer

这个分词器同上，，然后进一步将所有的大写转为小写

**工厂类**：solr.LowerCaseTokenizerFactory

**参数**：无

```xml
<analyzer>
  <tokenizer class="solr.LowerCaseTokenizerFactory"/>
</analyzer>
```

**输入**："I just LOVE my iPhone!"

**输出**："i", "just", "love", "my", "iphone"

### N-Gram Tokenizer

按照所传递的参数，分割出参数大小的n-gram式的分词

**工厂类**：solr.NGramTokenizerFactory

**参数**

- minGramSize：（整形，默认是1）最小的n-gram值，必须大于0。
- maxGramSize：（整形，默认是2）最大的n-gram值，必须大于等于minGramSize。

```xml
<analyzer>
  <tokenizer class="solr.NGramTokenizerFactory"/>
</analyzer>
```

**输入**："hey man"

**输出**："h", "e", "y", " ", "m", "a", "n", "he", "ey", "y ", " m", "ma", "an"

### Edge N-Gram Tokenizer

从边缘开始的n-gram分词

**工厂类**：solr.EdgeNGramTokenizerFactory

**参数**

- minGramSize：（整数，默认是1），最小的n-gram的值，必须大于0
- maxGramSize：（整数，默认是1），最大的n-gram的值，必须大于minGramSize
- side：（“front”或者“back”，默认是“front”）从开始或者结束的边缘开始进行n-gram的分词

#### 默认示例

```xml
<analyzer>
  <tokenizer class="solr.EdgeNGramTokenizerFactory"/>
</analyzer>
```

**输入**："babaloo"

**输出**："b"

#### 范围示例

```xml
<analyzer>
  <tokenizer class="solr.EdgeNGramTokenizerFactory" minGramSize="2"
maxGramSize="5"/>
</analyzer>
```

**输入**："babaloo"

**输出**："ba", "bab", "baba", "babal"

#### 从结尾开始的范围示例

```xml
<analyzer>
  <tokenizer class="solr.EdgeNGramTokenizerFactory" minGramSize="2" maxGramSize="5" side="back"/>
</analyzer>
```

**输入**："babaloo"

**输出**："oo", "loo", "aloo", "baloo"

### Path Hierarchy Tokenizer

分割出所有级别的路径

**工厂类**：solr.PathHierarchyTokenizerFactory

**参数**

- delimiter：（字符，无默认值）可以提供一个文件路径的分隔符，然后使用另一个字符替换它。
- replace：（字符，无默认值）提供一个替换字符

```xml
<fieldType name="text_path" class="solr.TextField" positionIncrementGap="100">
  <analyzer>
    <tokenizer class="solr.PathHierarchyTokenizerFactory" delimiter="\"
replace="/"/>
  </analyzer>
</fieldType>
```

**输入**："c:\usr\local\apache"

**输出**："c:", "c:/usr", "c:/usr/local", "c:/usr/local/apache"

### Regular Expression Pattern Tokenizer

**工厂类**：solr.PatternTokenizerFactory

**参数**：

- pattern：（必填）正则表达式
- group：返回正则表达式匹配到的特定group。-1代表正则表达式当做分隔符来使用。正整数代表命中相应的group。0代表命中全部正则表达式。

#### 把“0个或者多个空格，加上一个逗号，加上0个或者多个空格”当做分隔符

```xml
<analyzer>
  <tokenizer class="solr.PatternTokenizerFactory" pattern="\s*,\s*"/>
</analyzer>
```

**输入**："fee,fie, foe , fum, foo"

**输出**："fee", "fie", "foe", "fum", "foo"

#### 匹配至少一个英文字符，并且第一个是大写字符的词

```xml
<analyzer>
  <tokenizer class="solr.PatternTokenizerFactory" pattern="[A-Z][A-Za-z]*" group="0"/>
</analyzer>
```

**输入**："Hello. My name is Inigo Montoya. You killed my father. Prepare to die."

**输出**："Hello", "My", "Inigo", "Montoya", "You", "Prepare"

#### 以“SKU”，“Part”或者“Part Number”开始，后可以接一个冒号，然后一个空格，后接数字或者连字符，命中第三组，也即为数字和连字符的词。

```xml
<analyzer>
  <tokenizer class="solr.PatternTokenizerFactory"
pattern="(SKU|Part(\sNumber)?):?\s(\[0-9-\]+)" group="3"/>
</analyzer>
```

**输入**："SKU: 1234, Part Number 5678, Part: 126-987"

**输出**："1234", "5678", "126-987"

### UAX29 URL Email Tokenizer

以空格和标点符号当做分隔符的分词器。

- 点（.）后没有紧跟空格，则不会当做分隔符用，点会包含在词中
- 连字符也会被当做分隔符，除非连字符的任意一端有数字
- 识别域名，email地址，file://路径，http(s)://路径，ftp://路径，IPv4和IPv6路径

> 此分词器支持在[Unicode standard annex UAX#29](http://unicode.org/reports/tr29/#Word_Boundaries)定义中的&lt;ALPHANUM&gt;，&lt;NUM&gt;，&lt;URL&gt;，&lt;EMAIL&gt;，&lt;SOUTHEAST_ASIAN&gt;，&lt;IDEOGRAPHIC&gt;和&lt;HIRAGAN A&gt;的词边界

**工厂类**：solr.UAX29URLEmailTokenizerFactory

**参数**：

- maxTokenLength：（整数，默认255）超过最大长度的词会被忽略

```xml
<analyzer>
  <tokenizer class="solr.UAX29URLEmailTokenizerFactory"/>
</analyzer>
```

**输入**："Visit http://accarol.com/contact.htm?from=external&a=10 or e-mail bob.cratchet@accarol.com"

**输出**："Visit", "http://accarol.com/contact.htm?from=external&a=10", "or", "e", "mail", "bob.cratchet@accarol.com"

### White Space Tokenizer

只用空格做分隔符，返回空格分割后的字符

**工厂类**：solr.WhitespaceTokenizerFactory

**参数**：无

```xml
<analyzer>
  <tokenizer class="solr.WhitespaceTokenizerFactory"/>
</analyzer>
```

**输入**："To be, or what?"

**输出**："To", "be,", "or", "what?"

